{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import connectorx as cx\n",
    "\n",
    "print(\"GE version:\", gx.__version__)\n",
    "\n",
    "context = gx.get_context() \n",
    "print(f\"Context type: {type(context).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasource = context.data_sources.add_pandas(name=\"ashrae_pandas_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = datasource.add_dataframe_asset(name=\"my_runtime_asset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({\"a\": [1, 2, 3, None]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_definition = asset.add_batch_definition_whole_dataframe(\"my_batch_definition\")\n",
    "print(f\"Batch definition type: {type(batch_definition).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = context.get_validator(batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_not_be_null(\"a\")\n",
    "print(validator.validate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_suite = gx.ExpectationSuite(name=\"ashrae_quality_suite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = context.suites.add(new_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expectation_suite = suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_not_be_null(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Create a new DataFrame with a null (None or np.nan)\n",
    "df_with_nulls = pd.DataFrame({\"a\": [1, None, 3]})\n",
    "\n",
    "# 2. Get a NEW batch using this new DataFrame\n",
    "# (Since the data changed, we need a new batch object)\n",
    "new_batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df_with_nulls})\n",
    "\n",
    "# 3. Get a validator for this batch\n",
    "validator = context.get_validator(batch=new_batch)\n",
    "\n",
    "# 4. Run the expectation\n",
    "result = validator.expect_column_values_to_not_be_null(\"a\")\n",
    "\n",
    "# 5. Look at the results\n",
    "print(f\"Validation Success: {result.success}\")\n",
    "print(f\"Unexpected Count: {result.result['unexpected_count']}\")\n",
    "print(f\"Unexpected Values: {result.result['partial_unexpected_list']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "\n",
    "# 1. SETUP\n",
    "context = gx.get_context()\n",
    "\n",
    "# 2. DATA PREPARATION\n",
    "df = pd.DataFrame({\n",
    "    \"id\": [101, 102, 103, 104],\n",
    "    \"score\": [85.0, 92.0, np.nan, 88.0]\n",
    "})\n",
    "\n",
    "# 3. INFRASTRUCTURE (Robust checks for existing objects)\n",
    "datasource_name = \"production_pandas_datasource\"\n",
    "datasource = context.data_sources.add_or_update_pandas(name=datasource_name)\n",
    "\n",
    "asset_name = \"user_metrics_asset\"\n",
    "try:\n",
    "    asset = datasource.get_asset(asset_name)\n",
    "except LookupError:\n",
    "    asset = datasource.add_dataframe_asset(name=asset_name)\n",
    "\n",
    "batch_definition_name = \"whole_dataframe_definition\"\n",
    "try:\n",
    "    batch_definition = asset.get_batch_definition(batch_definition_name)\n",
    "except LookupError:\n",
    "    batch_definition = asset.add_batch_definition_whole_dataframe(batch_definition_name)\n",
    "\n",
    "# 4. SUITE: Create or get the suite\n",
    "suite_name = \"user_metrics_suite_2.0\"\n",
    "suite = context.suites.add_or_update(gx.ExpectationSuite(name=suite_name))\n",
    "\n",
    "# 5. VALIDATION: Get the Validator\n",
    "batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df})\n",
    "validator = context.get_validator(batch=batch, expectation_suite=suite)\n",
    "\n",
    "# --- Define Expectations ---\n",
    "validator.expect_column_values_to_not_be_null(column=\"id\")\n",
    "validator.expect_column_values_to_not_be_null(column=\"score\")\n",
    "validator.expect_column_values_to_be_between(column=\"score\", min_value=0, max_value=100)\n",
    "\n",
    "# 6. EXECUTION: Run validation\n",
    "validation_result = validator.validate()\n",
    "\n",
    "# FIX: Instead of validator.save_expectation_suite(), use the context factory directly\n",
    "# This pulls the expectations we just ran and saves/updates them in the context.\n",
    "context.suites.add_or_update(validator.get_expectation_suite())\n",
    "\n",
    "# 7. REPORTING\n",
    "print(f\"\\nValidation Report: {suite_name}\")\n",
    "print(f\"Status: {'‚úÖ SUCCESS' if validation_result.success else '‚ùå FAILED'}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for result in validation_result.results:\n",
    "    res_status = \"Pass\" if result.success else \"Fail\"\n",
    "    col = result.expectation_config.kwargs.get('column')\n",
    "    print(f\"[{res_status}] {col}: {result.expectation_config.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bishesh/miniconda3/envs/myenv/lib/python3.11/site-packages/great_expectations/expectations/expectation.py:1464: UserWarning: `result_format` configured at the Validator-level will not be persisted. Please add the configuration to your Checkpoint config or checkpoint_run() method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c20079acf145edbc5c301f72781ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bishesh/miniconda3/envs/myenv/lib/python3.11/site-packages/great_expectations/expectations/expectation.py:1464: UserWarning: `result_format` configured at the Validator-level will not be persisted. Please add the configuration to your Checkpoint config or checkpoint_run() method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8772b9d6c97b4e64a0db14de860c47a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bishesh/miniconda3/envs/myenv/lib/python3.11/site-packages/great_expectations/expectations/expectation.py:1464: UserWarning: `result_format` configured at the Validator-level will not be persisted. Please add the configuration to your Checkpoint config or checkpoint_run() method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a314e00735ac4872b798d46fd3342293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bishesh/miniconda3/envs/myenv/lib/python3.11/site-packages/great_expectations/expectations/expectation.py:1464: UserWarning: `result_format` configured at the Validator-level will not be persisted. Please add the configuration to your Checkpoint config or checkpoint_run() method instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9513cce3eb40f4a3ca148e16078c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5d69e631d94e2fad8b0c128a39f9ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Metrics:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä DETAILED VALIDATION REPORT: user_data_quality_suite\n",
      "Executed at: 2025-12-29 12:29:59\n",
      "================================================================================\n",
      "OVERALL STATUS      : ‚ùå FAILED\n",
      "SUCCESS RATE        : 50.00%\n",
      "EXPECTATIONS        : 4 total | 2 passed | 2 failed\n",
      "--------------------------------------------------------------------------------\n",
      "COLUMN          | EXPECTATION                         | STATUS\n",
      "--------------------------------------------------------------------------------\n",
      "id              | expect_column_values_to_not_be_null | ‚úÖ PASS\n",
      "score           | expect_column_values_to_not_be_null | ‚ùå FAIL\n",
      "   ‚îî‚îÄ ‚ö†Ô∏è  FAILURE DETAIL: 1/5 values failed (20.0%)\n",
      "   ‚îî‚îÄ üîç SAMPLE BAD DATA: [nan]\n",
      "\n",
      "score           | expect_column_values_to_be_between  | ‚úÖ PASS\n",
      "status          | expect_column_values_to_be_in_set   | ‚ùå FAIL\n",
      "   ‚îî‚îÄ ‚ö†Ô∏è  FAILURE DETAIL: 1/5 values failed (20.0%)\n",
      "   ‚îî‚îÄ üîç SAMPLE BAD DATA: ['unknown']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[INFO] Data Docs generated. Use 'context.open_data_docs()' to view the visual report.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as gx\n",
    "import datetime\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP: CONTEXT & DATA\n",
    "# ==========================================\n",
    "context = gx.get_context()\n",
    "\n",
    "# Create example data: 'id' is clean, 'score' has a null, 'status' has an invalid value\n",
    "df = pd.DataFrame({\n",
    "    \"id\": [101, 102, 103, 104, 105],\n",
    "    \"score\": [85.0, 92.0, np.nan, 88.0, 76.0],\n",
    "    \"status\": [\"active\", \"active\", \"pending\", \"inactive\", \"unknown\"]\n",
    "})\n",
    "\n",
    "# ==========================================\n",
    "# 2. INFRASTRUCTURE: DATASOURCE & ASSET\n",
    "# ==========================================\n",
    "datasource_name = \"production_datasource\"\n",
    "datasource = context.data_sources.add_or_update_pandas(name=datasource_name)\n",
    "\n",
    "asset_name = \"user_metrics_asset\"\n",
    "try:\n",
    "    asset = datasource.get_asset(asset_name)\n",
    "except LookupError:\n",
    "    asset = datasource.add_dataframe_asset(name=asset_name)\n",
    "\n",
    "batch_def_name = \"full_dataframe_batch\"\n",
    "try:\n",
    "    batch_definition = asset.get_batch_definition(batch_def_name)\n",
    "except LookupError:\n",
    "    batch_definition = asset.add_batch_definition_whole_dataframe(batch_def_name)\n",
    "\n",
    "# ==========================================\n",
    "# 3. SUITE & VALIDATOR\n",
    "# ==========================================\n",
    "suite_name = \"user_data_quality_suite\"\n",
    "# Create or Update the suite\n",
    "suite = context.suites.add_or_update(gx.ExpectationSuite(name=suite_name))\n",
    "\n",
    "# Get the batch of data\n",
    "batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df})\n",
    "\n",
    "# Initialize Validator\n",
    "validator = context.get_validator(batch=batch, expectation_suite=suite)\n",
    "\n",
    "# ==========================================\n",
    "# 4. DEFINE RULES (EXPECTATIONS)\n",
    "# ==========================================\n",
    "\n",
    "# Rule 1: 'id' must exist (PASS)\n",
    "validator.expect_column_values_to_not_be_null(column=\"id\")\n",
    "\n",
    "# Rule 2: 'score' must exist (FAIL - because of np.nan)\n",
    "validator.expect_column_values_to_not_be_null(column=\"score\")\n",
    "\n",
    "# Rule 3: 'score' must be between 0-100 (PASS)\n",
    "validator.expect_column_values_to_be_between(column=\"score\", min_value=0, max_value=100)\n",
    "\n",
    "# Rule 4: 'status' must be in a specific list (FAIL - \"unknown\" is not in list)\n",
    "validator.expect_column_values_to_be_in_set(\n",
    "    column=\"status\", \n",
    "    value_set=[\"active\", \"pending\", \"inactive\"]\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 5. PERSISTENCE (SAVE RULES)\n",
    "# ==========================================\n",
    "# Manual save to avoid \"Already Exists\" error in GX 1.x\n",
    "context.suites.add_or_update(validator.get_expectation_suite())\n",
    "\n",
    "# ==========================================\n",
    "# 6. EXECUTION & DETAILED REPORTING\n",
    "# ==========================================\n",
    "validation_result = validator.validate()\n",
    "\n",
    "def print_detailed_report(result, name):\n",
    "    stats = result.statistics\n",
    "    print(\"=\"*80)\n",
    "    print(f\"üìä DETAILED VALIDATION REPORT: {name}\")\n",
    "    print(f\"Executed at: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Summary Table\n",
    "    print(f\"{'OVERALL STATUS':<20}: {'‚úÖ PASSED' if result.success else '‚ùå FAILED'}\")\n",
    "    print(f\"{'SUCCESS RATE':<20}: {stats['success_percent']:.2f}%\")\n",
    "    print(f\"{'EXPECTATIONS':<20}: {stats['evaluated_expectations']} total | {stats['successful_expectations']} passed | {stats['unsuccessful_expectations']} failed\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Breakdown Table Header\n",
    "    print(f\"{'COLUMN':<15} | {'EXPECTATION':<35} | {'STATUS'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for res in result.results:\n",
    "        col = res.expectation_config.kwargs.get(\"column\", \"Table\")\n",
    "        etype = res.expectation_config.type\n",
    "        status = \"‚úÖ PASS\" if res.success else \"‚ùå FAIL\"\n",
    "        \n",
    "        print(f\"{str(col):<15} | {etype:<35} | {status}\")\n",
    "        \n",
    "        # detailed failure analysis\n",
    "        if not res.success:\n",
    "            details = res.result\n",
    "            count = details.get('unexpected_count', 0)\n",
    "            total = details.get('element_count', 0)\n",
    "            pct = details.get('unexpected_percent', 0)\n",
    "            bad_values = details.get('partial_unexpected_list', [])\n",
    "            \n",
    "            print(f\"   ‚îî‚îÄ ‚ö†Ô∏è  FAILURE DETAIL: {count}/{total} values failed ({pct:.1f}%)\")\n",
    "            if bad_values:\n",
    "                print(f\"   ‚îî‚îÄ üîç SAMPLE BAD DATA: {bad_values}\")\n",
    "            print(\"\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Print the custom console report\n",
    "print_detailed_report(validation_result, suite_name)\n",
    "\n",
    "# ==========================================\n",
    "# 7. VISUAL DATA DOCS (HTML)\n",
    "# ==========================================\n",
    "# This creates a professional website with graphs and details\n",
    "context.build_data_docs()\n",
    "# Uncomment the line below to automatically open the report in your browser\n",
    "# context.open_data_docs()\n",
    "\n",
    "print(\"\\n[INFO] Data Docs generated. Use 'context.open_data_docs()' to view the visual report.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
